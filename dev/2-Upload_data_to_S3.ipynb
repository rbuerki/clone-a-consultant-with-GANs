{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload directories to S3\n",
    "\n",
    "There is nothing in the boto3 library itself that would allow you to upload an entire directory. So I write my own code to traverse a directory tree using pathlib and upload each individual file using boto3.\n",
    "\n",
    "S3 is a key value store with a flat structure and technically does not have folders (although it supports the concept). That's why one works with prefix \"keys\" in file names like `abc/xys/uvw/123.jpg`.\n",
    "\n",
    "\n",
    "_Alternatively I could try some of these alternatives:_\n",
    "- The command line utility in boto called `s3put` that handles such operations\n",
    "- The AWS CLI tool has a lot of features that allow uploading entire directories or even [sync](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3/sync.html) the S3 bucket with a local directory or vice-versa. \n",
    "- A python filesystem library called `s3sf` that provides high-level functionality over boto3 and enables filesystem-like operations on s3.\n",
    "\n",
    "Helpful resources:\n",
    "- [boto3 documentation on s3](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-examples.html)\n",
    "- [S3 user guide on: How do I use folders in S3?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/using-folders.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T21:30:28.556042Z",
     "start_time": "2020-09-25T21:30:28.550954Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T21:30:29.265084Z",
     "start_time": "2020-09-25T21:30:29.260078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\r2d4\\miniconda3\\envs\\pytorch\\python.exe\n",
      "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "print(sys.executable)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T21:30:30.531763Z",
     "start_time": "2020-09-25T21:30:29.934190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clone-a-consultant-20-09\n",
      "elasticbeanstalk-us-east-2-873674308518\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the s3 client\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Check my existing buckets\n",
    "my_buckets = s3.list_buckets()\n",
    "for bucket in my_buckets[\"Buckets\"]:\n",
    "    print(bucket[\"Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T21:30:36.610962Z",
     "start_time": "2020-09-25T21:30:36.604964Z"
    }
   },
   "outputs": [],
   "source": [
    "# If not exists create a new bucket for the project\n",
    "new_bucket_name = \"clone-a-consultant-20-09\"\n",
    "if not new_bucket_name in [bucket[\"Name\"] for bucket in my_buckets[\"Buckets\"]]:  \n",
    "    response = s3.create_bucket(\n",
    "        Bucket=new_bucket_name,\n",
    "        CreateBucketConfiguration={\n",
    "            'LocationConstraint': 'eu-west-1',\n",
    "        },\n",
    "    )\n",
    "    print(response[\"location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T21:30:37.570454Z",
     "start_time": "2020-09-25T21:30:37.564909Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_file_generator(path):\n",
    "    return Path(path).rglob(\"*.*\")\n",
    "\n",
    "\n",
    "def upload_file_to_s3(local_path, client, bucket, s3_dir=False):\n",
    "    if s3_dir:\n",
    "        s3_path = str(Path(s3_dir) / local_path).replace(\"\\\\\", \"/\")\n",
    "    else: \n",
    "        s3_path = str(local_path).replace(\"\\\\\", \"/\")\n",
    "    # Check if file already exists, if yes skip, if no upload\n",
    "    try:\n",
    "        client.head_object(Bucket=bucket, Key=s3_path)\n",
    "        print(f\"File found in s3 bucket! Skipping {s3_path}\")\n",
    "    except:\n",
    "#         print(f\"Uploading {s3_path} ...\")\n",
    "#         s3.upload_file(local_path, bucket, s3_path) \n",
    "        print(local_path)\n",
    "        print(f\"{s3_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T21:30:39.098772Z",
     "start_time": "2020-09-25T21:30:38.797658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\train\\test.txt\n",
      "extra/extra/data/train/test.txt\n",
      "\n",
      "data\\train2\\test.txt\n",
      "extra/extra/data/train2/test.txt\n",
      "\n",
      "data\\train3\\test.txt\n",
      "extra/extra/data/train3/test.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gener = create_file_generator(r\"data\")\n",
    "\n",
    "for local_path in gener:\n",
    "    upload_file_to_s3(local_path, s3, new_bucket_name, s3_dir=\"extra\\extra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the s3 docs\n",
    "\n",
    "\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
