{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T14:14:10.744078Z",
     "start_time": "2020-10-18T14:14:10.607070Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pickle\n",
    "import sys\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T14:14:11.226188Z",
     "start_time": "2020-10-18T14:14:11.079076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('raph-base')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "pd.set_option('precision', 2)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T14:14:11.580189Z",
     "start_time": "2020-10-18T14:14:11.454192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\r2d4\\miniconda3\\envs\\py3\\python.exe\n",
      "3.8.3 (default, May 19 2020, 06:50:17) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "print(sys.executable)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Check Text Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T14:14:12.861355Z",
     "start_time": "2020-10-18T14:14:12.717900Z"
    }
   },
   "outputs": [],
   "source": [
    "# load text data\n",
    "\n",
    "def load_text_data(path: Path, encoding: str=\"UTF-8\") -> str:\n",
    "    \"\"\"Load textcorpus from file into a string.\n",
    "    Replace double line breaks with simple break.\n",
    "    \"\"\"\n",
    "    text_path = Path(path)\n",
    "    with text_path.open(mode='r'):\n",
    "        corpus = text_path.read_text(encoding=encoding)\n",
    "        corpus = corpus.replace(\"\\n\\n\", \"\\n\")\n",
    "        return corpus\n",
    "\n",
    "corpus = load_text_data(\"../data/training/text/text_corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T14:14:13.405641Z",
     "start_time": "2020-10-18T14:14:13.141166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 282,312\n",
      "Approx. number of unique words: 21,947\n",
      "Number of lines: 1,836\n",
      "Average number of words in each line: 154\n"
     ]
    }
   ],
   "source": [
    "# Print some stats\n",
    "print(f\"Total number of words: {len(corpus.split()):,.0f}\")\n",
    "print(f\"Approx. number of unique words: {len({word: None for word in corpus.split()}):,.0f}\")\n",
    "\n",
    "lines = corpus.split('\\n')\n",
    "print(f\"Number of lines: {len(lines):,.0f}\")\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print(f\"Average number of words in each line: {round(np.average(word_count_line),0):,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T14:14:13.668041Z",
     "start_time": "2020-10-18T14:14:13.533520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The lines 0 to 3:\n",
      "\n",
      "Robin Dreyer is a management consultant in Implement’s commercial excellence team in Zurich. Robin has experience with strategic marketing as well as sales planning and execution and specialise in automated digital marketing and lead management. Before joining Implement, Robin worked for swiss watch manufacturer IWC in the marketing department, where he was responsible for public relations monitoring and several strategic initiatives. During his time at Tesla, he gained first-hand sales experience and supported the setup and launch of a CRM system.\n",
      "\n",
      "Tesla Inc., Sales Intern (Hamburg) (2017) Custom Programs (ES-HSG), Student Assistant (2015-2016) IWC Schaffhausen, Marketing Intern (2014-2015)\n",
      "\n",
      "CEMS M.S. in International Management, IVEY Business School, London Ontario (2017) MA in Business Management, University of St. Gallen (2016) BA in Business Administration, University of St. Gallen (2014)\n",
      "\n",
      "Professional services (2019-): Aligning and structuring service level agreements. The goal of this project is to ensure that the legally binding service level agreements have been consolidated and an operating process to ensure that they are being fulfilled. The service level matrix was digitised and is running automated in the background to track and report every single service level of all tickets that have been created in terms of duration and quality. Role: consultant. Kompetenzzentrum Integration – Stadt Bern (2018-2019): Restructuring asylum seeking/integration process. Project management and support of a public bid on our client’s behalf. The goal was to development an efficient integration process for asylum seekers, including financial modelling and key performance indicators. The bid has been delivered and is now being evaluated. Role: consultant. Manufacturing (2018-2019): Webshop development. Project management of webshop creation (facilitation, testing and controlling). In this project, we helped our client to set up a new webshop, where cleaning services could be ordered online by their customers. The number of items and complexity was substantial as there were pre-set agreements in terms of when what had to be cleaned in which rhythm. The project will continue as other key accounts have shown interest and our client is planning to extend the offering. Role: consultant. Manufacturing (2018): Boost sales excellence. Development of a structured sales process, sales tools and training of sales force. The initial phase focused on crafting a strategy that fitted the German market, but one that could later be applied to other European markets. A key initiative was to set up a CRM system in which the local sales force could take notes, track the progress and define the crucial next steps. Role: consultant.\n"
     ]
    }
   ],
   "source": [
    "# Print some sample lines\n",
    "\n",
    "VIEW_LINE_RANGE = (0, 3)\n",
    "\n",
    "print(\"\\nThe lines {} to {}:\\n\".format(*VIEW_LINE_RANGE))\n",
    "print(\"\\n\\n\".join(corpus.split(\"\\n\")[VIEW_LINE_RANGE[0]:VIEW_LINE_RANGE[1]+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build pre-processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T14:14:14.716764Z",
     "start_time": "2020-10-18T14:14:14.582653Z"
    }
   },
   "outputs": [],
   "source": [
    "TOKEN_LOOKUP = {\".\": \"<PERIOD>\",\n",
    "                \",\": \"<COMMA>\",\n",
    "                '\"': \"<QUOTDOUBLE>\",\n",
    "                \"'\": \"<QUOTSINGLE>\",\n",
    "                \":\": \"<COLON>\",\n",
    "                \";\": \"<SEMICOLON>\" ,\n",
    "                \"!\": \"<EXCLAMATIONMARK>\",\n",
    "                \"?\": \"<QUESTIONMARK>\",\n",
    "                \"(\": \"<LEFTPAREN>\",\n",
    "                \")\": \"<RIGHTPAREN>\",\n",
    "                \"-\": \"<DASH>\",\n",
    "                \"?\": \"<QUESTIONMARK>\",\n",
    "                \"\\n\": \"<NEWLINE>\",\n",
    "                \"_\": \"<UNDERSCORE>\",\n",
    "                \"PADDING\": '<PAD>'\n",
    "                }\n",
    "\n",
    "VIEW_LINE_RANGE = (0, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T14:26:11.880221Z",
     "start_time": "2020-10-18T14:26:11.736978Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_text_data(path: str, encoding: str=\"UTF-8\") -> str:\n",
    "    \"\"\"Load textcorpus from file into a string.\n",
    "    Replace double line breaks with simple break.\n",
    "    \"\"\"\n",
    "    text_path = Path(path)\n",
    "    with text_path.open(mode='r'):\n",
    "        text = text_path.read_text(encoding=encoding)\n",
    "        text = text.replace(\"\\n\\n\", \"\\n\")\n",
    "        return text\n",
    "\n",
    "    \n",
    "def print_some_text_stats(text: str):\n",
    "    \"\"\"Output some metrics for information.\"\"\"\n",
    "    print(f\"Approx. total number of words: {len(corpus.split()):,.0f}\")\n",
    "    print(f\"Approx. number of unique words: {len({word: None for word in corpus.split()}):,.0f}\")\n",
    "    lines = corpus.split('\\n')\n",
    "    print(f\"Number of lines: {len(lines):,.0f}\")\n",
    "    word_count_line = [len(line.split()) for line in lines]\n",
    "    print(f\"Average number of words in each line: {round(np.average(word_count_line),0):,.0f}\\n\\n\")\n",
    "\n",
    "\n",
    "def print_some_sample_lines(text: str, view_line_range: Tuple[int, int]=None):\n",
    "    \"\"\"Output some lines for information. This step is skipped by default.\n",
    "    To activate it a tuple with a view range is explicitely passed.\n",
    "    \"\"\"\n",
    "    if view_line_range:\n",
    "        print(\"\\nThe lines {} to {}:\\n\".format(*VIEW_LINE_RANGE))\n",
    "        print(\"\\n\\n\".join(text.split(\"\\n\")[VIEW_LINE_RANGE[0]:VIEW_LINE_RANGE[1]+1]))\n",
    "\n",
    "\n",
    "def clean_text(text: str, lookup: Dict[str, str]=TOKEN_LOOKUP) -> List[str]:\n",
    "    \"\"\"Normalize the text and convert the special characters with help of \n",
    "    a lookup dictionary, thereby ensuring that they are separated from the\n",
    "    actual words. So we can properly split the text and return a list of\n",
    "    all the words.\"\"\"\n",
    "    for special_char, token in TOKEN_LOOKUP.items():\n",
    "        text = text.replace(special_char, f\" {token} \")\n",
    "        text = text.lower()\n",
    "        # Add a special word that we will use later on\n",
    "        text = \"\".join([text, \" <PAD>\"])\n",
    "        return text.split()\n",
    "\n",
    "\n",
    "def create_encoding_dicts(text_list: List[str]) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"Create two encoding dicts for the vocabulary int the text \n",
    "    (str2int, int2str). The vocabulary is sorted in descending by \n",
    "    frequency.\n",
    "    \"\"\"\n",
    "    # Get a list of unique words sorted by frequency using Counter()\n",
    "    word_counts = Counter(text_list)\n",
    "    word_list_sorted = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    \n",
    "    vocab_to_int = {word: pos for pos, word in enumerate(word_list_sorted)}\n",
    "    int_to_vocab = {pos: word for pos, word in enumerate(word_list_sorted)}\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "\n",
    "def encode_text_to_int(text_list: List[str], vocab_to_int: Dict[str, int]) -> List[int]:\n",
    "    \"\"\"Translate the cleaned text (list) into a list of integers using\n",
    "    the encoding dict. This will be the text corpus for training the NN.\n",
    "    \"\"\"\n",
    "    return [vocab_to_int[word] for word in text_list]\n",
    "\n",
    "def pickle_preprocessed_text_data(path: str, data_objects: Iterable[Any]):\n",
    "    \"\"\"Save the necessary objects for the later steps in one single\n",
    "    binary pickle file.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    with open(path, \"wb\") as p_file: \n",
    "#         for obj in data_objects:\n",
    "        pickle.dump(data_objects, p_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T14:26:12.877195Z",
     "start_time": "2020-10-18T14:26:12.390200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx. total number of words: 282,312\n",
      "Approx. number of unique words: 21,947\n",
      "Number of lines: 1,836\n",
      "Average number of words in each line: 154\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = load_text_data(\"../data/training/text/text_corpus.txt\")\n",
    "print_some_text_stats(text)\n",
    "print_some_sample_lines(text)\n",
    "text_list =clean_text(text)\n",
    "vocab_to_int, int_to_vocab = create_encoding_dicts(text_list)\n",
    "int_text = encode_text_to_int(text_list, vocab_to_int)\n",
    "pickle_preprocessed_text_data(\"preprocessed_text_data.pkl\", \n",
    "                              [int_text,\n",
    "                               vocab_to_int,\n",
    "                               int_to_vocab,\n",
    "                               TOKEN_LOOKUP\n",
    "                               ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
